# Fault-Aware Distillation via Peer-Review (FAIR) approach
Code for the paper: "[Learning from Committee: Reasoning Distillation from a Mixture of Teachers with Peer-Review](https://arxiv.org/abs/2410.03663)", accepted by ACL 2025.
![Overview of our FAIR method.](procedure.png)

## Before You Start
This repo aims to reproduce our paper's work, so the supportive models and datasets are derived from the paper's experimental settings. Since our work is based on public models, datasets, training frameworks on HuggingFace and official APIs, it is **easy to scale and adapt**. Feel free to clone and customize this repository for your own tasks!

- Backbone student model: [Llama2-7B-chat](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)  
- Teacher models: [GPT-3.5-Turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo), [Gemini-1.0-Pro](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemini-pro?inv=1&invt=Abx3xA), [Mixtral-8x7B-Instructv0.1](https://deepinfra.com/mistralai/Mixtral-8x7B-Instruct-v0.1)  
- Datasets: [GSM8K](https://huggingface.co/datasets/openai/gsm8k), [SVAMP](https://huggingface.co/datasets/ChilleD/SVAMP), [StrategyQA](https://huggingface.co/datasets/ChilleD/StrategyQA), [LogiQA](https://huggingface.co/datasets/lucasmccabe/logiqa)  
In the command line, they are shown as: `parser.add_argument('--dataset', default='strategyQA', choices=['gsm8k', 'svamp', 'strategyQA', 'logiQA'], help="which dataset")`

Different folders explanation:
- `/local_models`: The `cache_dir` of `AutoModelForCausalLM.from_pretrained()` for downloaded student models.
- `/local_dataset`: The `cache_dir` of `load_dataset()` for downloaded datasets.
- `/data`: The folder to store processed data, such as wrong answers from student models, rationales and feedback generated by teacher models.  
To support the open-source community, we have provided four JSON files in `/data` containing the incorrect answers generated by the Llama2-7B-chat model on four benchmark datasets.
- `/checkpoints`: The `output_dir` for trained student models.
