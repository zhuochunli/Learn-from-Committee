# Fault-Aware Distillation via Peer-Review (FAIR) approach
Code for the paper: "[Learning from Committee: Reasoning Distillation from a Mixture of Teachers with Peer-Review](https://arxiv.org/abs/2410.03663)", accepted by ACL 2025.
![Overview of our FAIR method.](procedure.png)

## Before You Start
This repo aims to reproduce our paper's work, so the supportive models and datasets are derived from the paper's experimental settings. Since our work is based on public models, datasets, training frameworks on HuggingFace and official APIs, it is **easy to scale and adapt**. Feel free to clone and customize this repository for your own tasks!

- Backbone student model: [Llama2-7B-chat](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)  
- Teacher models: [GPT-3.5-Turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo), [Gemini-1.0-Pro](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemini-pro?inv=1&invt=Abx3xA), [Mixtral-8x7B-Instructv0.1](https://deepinfra.com/mistralai/Mixtral-8x7B-Instruct-v0.1)  
- Datasets: [GSM8K](https://huggingface.co/datasets/openai/gsm8k), [SVAMP](https://huggingface.co/datasets/ChilleD/SVAMP), [StrategyQA](https://huggingface.co/datasets/ChilleD/StrategyQA), [LogiQA](https://huggingface.co/datasets/lucasmccabe/logiqa)  
In the command line, they are shown as: `parser.add_argument('--dataset', default='strategyQA', choices=['gsm8k', 'svamp', 'strategyQA', 'logiQA'], help="which dataset")`

Directory explanation:
- `/local_models`: The `cache_dir` of `AutoModelForCausalLM.from_pretrained()` for downloaded student models.
- `/local_dataset`: The `cache_dir` of `load_dataset()` for downloaded datasets.
- `/data`: The folder to store processed data, such as wrong answers from student models, rationales and feedback generated by teacher models.  
To support the open-source community, we have provided four JSON files in `/data` containing the incorrect answers generated by the Llama2-7B-chat model on four benchmark datasets.
- `/checkpoints`: The `output_dir` for trained student models.

## Quick Start
### 1. Requirements
`pip install -r requirements.txt`
### 2. Collecting Mistakes on Student Model
You can select different student models from Huggingface and datasets from ['gsm8k', 'svamp', 'strategyQA', 'logiQA']. For example:  
`python infer_student_wrong.py -model meta-llama/Llama-2-7b-chat-hf --dataset gsm8k`  

After running, the student model will be downloaded under `/local_models` and datasets will be downloaded under `/local_dataset`. The student's wrong answers will be stored under `/data`, like `Llama-2-7b-chat-hf_gsm8k_false_round0.json`.
### 3. Inquiring Teacher LLMs with Studentsâ€™ Mistakes
First, you can set your API key for GPT-3.5-Turbo, Gemini-1.0-Pro, and Mixtral-8x7B-Instructv0.1. For example:  
`export OPENAI_API_KEY='Your key'`  
`export GEMINI_API_KEY='Your key'`  
`export MISTRAL_API_KEY='Your key'`  
Or, you can set the API keys when running the command:  
`python collect_teacher_res.py --student_wrong "data/Llama-2-7b-chat-hf_gsm8k_false_round0.json" --gpt_api "Your key" --mistral_api "Your key" --gemini_api "Your key"`  

After running, the feedback generated by three different teacher LLMs and rationales collected via the peer-review process will be stored under `/data`. For example: `Llama-2-7b-chat-hf_gsm8k_feedback_round0.json` and `Llama-2-7b-chat-hf_gsm8k_rationale_round0.json`.


